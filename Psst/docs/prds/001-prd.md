# PRD: AI Backend Infrastructure

**Feature**: AI Backend Infrastructure (Pinecone + OpenAI + Cloud Functions)

**Version**: 1.0

**Status**: Draft

**Agent**: Pam

**Target Release**: Phase 1 - Foundation

**Links**: [PR Brief: ai-briefs.md PR #001], [TODO: pr-001-todo.md]

---

## 1. Summary

Establish the foundational AI infrastructure for Psst by integrating Pinecone vector database, OpenAI embedding API, and Firebase Cloud Functions to automatically generate and store vector embeddings for all chat messages. This invisible backend infrastructure enables future semantic search and RAG capabilities without any user-facing changes.

---

## 2. Problem & Goals

**Problem:** Trainers need AI-powered semantic search to recall client conversations ("What did Sarah say about her diet?"), but this requires vector embeddings of all messages stored in a searchable database.

**Why Now:** This is Phase 1 foundation work. Without this infrastructure, no AI features (chat assistant, contextual actions, proactive suggestions) can function.

**Goals:**
- [ ] G1 â€” Enable automatic vector embedding generation for all messages sent through Psst
- [ ] G2 â€” Store embeddings in Pinecone vector database with proper indexing for semantic search
- [ ] G3 â€” Maintain < 200ms overhead for embedding generation without blocking message delivery

---

## 3. Non-Goals / Out of Scope

- [ ] No user-facing UI changes (foundation only)
- [ ] No semantic search implementation (that's PR #5)
- [ ] No RAG pipeline integration (that's PR #5)
- [ ] No embedding generation for existing messages (historical backfill is future work)
- [ ] No cost optimization or batch processing (MVP uses real-time generation)

---

## 4. Success Metrics

**User-visible:**
- No impact to message delivery latency (users don't notice embedding generation happening)
- Message sending remains < 100ms to Firestore

**System:**
- Embedding generation: < 200ms per message (OpenAI API call)
- Pinecone upsert: < 100ms per vector
- Cloud Function cold start: < 3s (warm execution < 500ms)
- 99% success rate for embedding generation (1% acceptable failure for retries)

**Quality:**
- 0 blocking bugs preventing message delivery
- Embedding generation failures don't break messaging
- All Cloud Functions properly error-handled and logged

---

## 5. Users & Stories

- As a **backend system**, I want to generate vector embeddings for every message so that future AI features can perform semantic search.
- As a **developer**, I want embeddings stored in Pinecone with proper metadata so that I can query them by userId, chatId, and timestamp.
- As a **trainer** (end user), I want message sending to remain fast so that I don't notice any backend AI processing.

---

## 6. Experience Specification (UX)

**Entry Points:**
- Triggered automatically when any message is sent via `MessageService.sendMessage()`
- No user interaction required (invisible infrastructure)

**Visual Behavior:**
- No UI changes
- Messages send exactly as they do today
- No loading states or indicators for embedding generation

**Performance:**
- Message delivery must not be blocked by embedding generation
- Embedding happens asynchronously after message is persisted to Firestore
- See targets in `Psst/agents/shared-standards.md` for messaging performance

---

## 7. Functional Requirements (Must/Should)

**MUST:**
- MUST generate embeddings for all new messages sent after this PR ships
- MUST use OpenAI `text-embedding-3-small` model (1536 dimensions)
- MUST store embeddings in Pinecone with cosine similarity metric
- MUST include metadata: `messageId`, `chatId`, `senderId`, `timestamp`, `text`
- MUST not block message delivery if embedding generation fails
- MUST log all errors for debugging (Cloud Functions logs)
- MUST use secure environment variables for API keys (OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENV)

**SHOULD:**
- SHOULD retry failed embedding generation up to 3 times with exponential backoff
- SHOULD batch Pinecone upserts when possible for efficiency (future optimization)

**Acceptance Gates:**
- [Gate] When User A sends message â†’ Message persists to Firestore immediately (< 100ms)
- [Gate] When message persisted â†’ Cloud Function triggers within 1 second
- [Gate] When Cloud Function executes â†’ Embedding generated in < 200ms
- [Gate] When embedding generated â†’ Pinecone stores vector in < 100ms
- [Gate] When embedding fails â†’ Message still delivered, error logged, retry scheduled

---

## 8. Data Model

### Firestore Schema (No Changes)
Messages continue to be stored in Firestore exactly as they are today:

```swift
// Firestore: /chats/{chatId}/messages/{messageId}
{
  id: String,
  text: String,
  senderID: String,
  timestamp: Timestamp,
  readBy: [String]
}
```

### Pinecone Vector Schema (New)
Embeddings stored in Pinecone index:

```typescript
// Pinecone Index: "coachai"
{
  id: messageId,  // Unique message ID from Firestore
  values: [Float],  // 1536-dimension vector from OpenAI
  metadata: {
    chatId: String,
    senderId: String,
    timestamp: Number,  // Unix timestamp
    text: String  // Original message text for search results
  }
}
```

**Pinecone Configuration:**
- Index name: `coachai`
- Dimensions: 1536
- Metric: `cosine`
- Pod type: `p1.x1` (starter tier - can scale later)
- Cloud: GCP (matches Firebase region)

**Validation Rules:**
- Pinecone index created before deployment
- API keys stored in Firebase Functions environment config
- No embedding for empty messages (skip if `text.trim().length === 0`)

**Indexing/Queries:**
- Firestore triggers on message creation: `/chats/{chatId}/messages/{messageId}`
- No additional Firestore indexes needed (no new queries)

---

## 9. API / Service Contracts

### Cloud Function: `generateEmbedding`

**Trigger:** Firestore onCreate for `/chats/{chatId}/messages/{messageId}`

**Input (from Firestore trigger):**
```typescript
{
  messageId: String,
  chatId: String,
  text: String,
  senderID: String,
  timestamp: Timestamp
}
```

**Processing:**
1. Validate message has non-empty text
2. Call OpenAI API to generate embedding
3. Upsert embedding to Pinecone with metadata
4. Log success/failure

**Output (Pinecone upsert):**
```typescript
{
  id: messageId,
  values: [1536 floats],
  metadata: { chatId, senderId, timestamp, text }
}
```

**Error Handling:**
- OpenAI timeout (30s) â†’ Retry with exponential backoff (1s, 2s, 4s)
- OpenAI rate limit (429) â†’ Retry after delay specified in response headers
- Pinecone timeout â†’ Retry up to 3 times
- Invalid message data â†’ Log error, skip embedding, don't retry
- API key missing â†’ Log critical error, alert developers

**Pre-conditions:**
- Message exists in Firestore
- Message has non-empty text field
- OpenAI and Pinecone API keys configured

**Post-conditions:**
- Embedding stored in Pinecone (success case)
- Error logged (failure case)
- No impact to message delivery

---

## 10. UI Components to Create/Modify

**No UI changes for this PR.** All work is backend infrastructure.

### Backend Files to Create:

- `functions/generateEmbedding.ts` â€” Cloud Function for embedding generation
- `functions/services/openaiService.ts` â€” Wrapper for OpenAI API calls
- `functions/services/pineconeService.ts` â€” Wrapper for Pinecone operations
- `functions/config/ai.config.ts` â€” AI service configuration (model names, dimensions, etc.)
- `functions/utils/retryHelper.ts` â€” Retry logic with exponential backoff

### Configuration Files to Modify:

- `functions/package.json` â€” Add dependencies: `openai`, `@pinecone-database/pinecone`
- `.env.example` â€” Document required environment variables

### Documentation to Create:

- `Psst/docs/ai-backend-setup.md` â€” Setup instructions for Pinecone, OpenAI, environment variables

---

## 11. Integration Points

**Firebase:**
- Firestore trigger for message creation
- Cloud Functions for serverless execution
- Environment config for secure API key storage

**OpenAI:**
- API endpoint: `https://api.openai.com/v1/embeddings`
- Model: `text-embedding-3-small`
- Authentication: Bearer token (OPENAI_API_KEY)

**Pinecone:**
- API endpoint: `https://api.pinecone.io`
- Index: `coachai`
- Authentication: API key (PINECONE_API_KEY)

**State Management:**
- No iOS state changes (backend only)

---

## 12. Testing Plan & Acceptance Gates

**See `Psst/docs/testing-strategy.md` for examples and detailed guidance.**

---

### Happy Path

- [ ] User sends message "Hey, how's your knee feeling?" in chat
- [ ] Message persists to Firestore immediately
- [ ] Cloud Function triggers within 1 second
- [ ] Embedding generated via OpenAI API
- [ ] Embedding stored in Pinecone with correct metadata
- **Gate:** Pinecone query returns the message with cosine similarity 1.0 (exact match)
- **Pass Criteria:** Flow completes without errors, embedding searchable in Pinecone

---

### Edge Cases

- [ ] **Edge Case 1: Empty message**
  - **Test:** Send message with empty text (`""` or whitespace only)
  - **Expected:** Cloud Function skips embedding generation, logs info message
  - **Pass:** No embedding created, no errors thrown, message still delivered

- [ ] **Edge Case 2: Very long message (1000+ characters)**
  - **Test:** Send 1000-character message (OpenAI limit is ~8000 tokens)
  - **Expected:** Embedding generated successfully, no truncation
  - **Pass:** Full text embedded, searchable in Pinecone

- [ ] **Edge Case 3: Special characters and emojis**
  - **Test:** Send message "Great job! ðŸ’ªðŸ”¥ Keep it up ðŸŽ¯"
  - **Expected:** Embedding includes emoji context, stored correctly
  - **Pass:** Embedding generated, metadata preserves emojis

---

### Error Handling

- [ ] **OpenAI API Timeout**
  - **Test:** Simulate slow OpenAI response (mock 30s+ delay)
  - **Expected:** Cloud Function retries with exponential backoff, logs timeout
  - **Pass:** Retry succeeds or fails gracefully after 3 attempts, message delivery unaffected

- [ ] **OpenAI Rate Limit (429)**
  - **Test:** Trigger rate limit by sending many messages rapidly (or mock 429 response)
  - **Expected:** Cloud Function respects retry-after header, queues retries
  - **Pass:** Embeddings eventually generated, no failed embeddings, clear logs

- [ ] **Pinecone Connection Failure**
  - **Test:** Temporarily disable Pinecone or use invalid API key
  - **Expected:** Cloud Function logs error, retries up to 3 times, then fails gracefully
  - **Pass:** Message delivery works, error logged for monitoring, alert triggered

- [ ] **Missing API Keys**
  - **Test:** Deploy Cloud Function without OPENAI_API_KEY configured
  - **Expected:** Function logs critical error on first message
  - **Pass:** Clear error message in logs, developers alerted, doesn't crash repeatedly

---

### Backend Verification

- [ ] **Cloud Function Deployment**
  - **Test:** Deploy `generateEmbedding` function, check Firebase Console
  - **Expected:** Function deployed successfully, shows green status
  - **Pass:** No deployment errors, logs accessible

- [ ] **Pinecone Index Created**
  - **Test:** Verify index exists in Pinecone dashboard
  - **Expected:** Index `coachai` exists with 1536 dimensions, cosine metric
  - **Pass:** Index configured correctly, ready for upserts

- [ ] **Environment Variables Set**
  - **Test:** Check Firebase Functions config for API keys
  - **Expected:** `firebase functions:config:get` shows all required keys
  - **Pass:** OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENV all set

---

### Performance Check

- [ ] Message delivery latency remains < 100ms (embedding happens async, doesn't block)
- [ ] Embedding generation completes in < 200ms (OpenAI API call)
- [ ] Pinecone upsert completes in < 100ms
- [ ] Cloud Function warm execution < 500ms (cold start < 3s acceptable)

---

## 13. Definition of Done

- [ ] Cloud Functions implemented (`generateEmbedding`, OpenAI service, Pinecone service)
- [ ] Retry logic with exponential backoff for API failures
- [ ] Error handling for all failure modes (timeouts, rate limits, invalid data)
- [ ] Pinecone index created and configured (1536 dimensions, cosine metric)
- [ ] Environment variables configured (OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENV)
- [ ] Manual testing completed (happy path, edge cases, error scenarios)
- [ ] All acceptance gates pass
- [ ] Documentation created (setup guide, environment variables)
- [ ] No impact to message delivery latency
- [ ] Logs clean and informative (success/failure clearly indicated)

---

## 14. Risks & Mitigations

**Risk: OpenAI API costs escalate with high message volume**
- Mitigation: Start with low-volume testing, monitor costs in OpenAI dashboard, set billing alerts at $50/month threshold

**Risk: Pinecone storage costs grow unbounded**
- Mitigation: Use starter tier (p1.x1), monitor index size, plan for data retention policy (future work)

**Risk: Embedding generation failures cause message loss**
- Mitigation: Decouple embedding generation from message delivery (async), implement retry logic, log all failures for recovery

**Risk: Cold start latency affects first message after idle period**
- Mitigation: Accept 3s cold start (Cloud Functions limitation), consider keep-warm strategy in future (scheduled ping)

**Risk: API keys exposed in code or logs**
- Mitigation: Use Firebase Functions environment config (not .env files in repo), never log API keys, use .gitignore for sensitive files

---

## 15. Rollout & Telemetry

**Feature Flag:** No (backend infrastructure, not user-facing)

**Metrics to Track:**
- Embedding generation success rate (target: 99%)
- Embedding generation latency (target: < 200ms)
- Pinecone upsert latency (target: < 100ms)
- Cloud Function error rate (target: < 1%)
- OpenAI API costs per day (monitor and alert at $5/day)
- Pinecone storage usage (GB and vector count)

**Manual Validation Steps:**
1. Send test message in Psst app
2. Check Firestore: message exists
3. Check Cloud Functions logs: `generateEmbedding` executed successfully
4. Check Pinecone dashboard: vector count increased by 1
5. Query Pinecone API: retrieve embedding by messageId
6. Verify metadata matches original message

**Rollout Plan:**
- Deploy to staging environment first
- Test with 10-20 messages manually
- Monitor logs and Pinecone for 24 hours
- Deploy to production
- Monitor for 7 days before enabling dependent features

---

## 16. Open Questions

- Q1: Should we generate embeddings for existing messages (backfill historical data)?
  - **Decision Needed By:** Before PR #5 (RAG pipeline) - if yes, create separate backfill script
  
- Q2: What's the message volume estimate for cost planning?
  - **Assumptions:** ~1000 messages/day initially (10 trainers x 100 messages/day), ~$0.10/day OpenAI costs
  
- Q3: Should we implement rate limiting on our side to prevent OpenAI quota issues?
  - **Decision:** Not in MVP - rely on OpenAI's rate limits and retry logic for now

---

## 17. Appendix: Out-of-Scope Backlog

Items deferred for future:
- [ ] Backfill embeddings for existing messages (separate migration script)
- [ ] Batch processing for efficiency (process multiple messages per API call)
- [ ] Cost optimization (use cheaper embedding models, cache common phrases)
- [ ] Keep-warm strategy for Cloud Functions (reduce cold starts)
- [ ] Data retention policy (delete old embeddings after N months)
- [ ] Multi-language embedding support (currently English-only)

---

## Preflight Questionnaire

1. **Smallest end-to-end user outcome for this PR?**
   - Backend can generate and store embeddings for messages (no user-visible changes)

2. **Primary user and critical action?**
   - Backend system, critical action: embed message text when message created

3. **Must-have vs nice-to-have?**
   - Must: Embedding generation, Pinecone storage, error handling
   - Nice: Batch processing, keep-warm, historical backfill

4. **Real-time requirements?**
   - Async processing (doesn't block message delivery), < 200ms embedding generation

5. **Performance constraints?**
   - No impact to message latency, < 200ms OpenAI call, < 100ms Pinecone upsert

6. **Error/edge cases to handle?**
   - Empty messages, API timeouts, rate limits, connection failures, missing API keys

7. **Data model changes?**
   - New Pinecone index (not Firestore changes)

8. **Service APIs required?**
   - OpenAI Embeddings API, Pinecone Upsert API

9. **UI entry points and states?**
   - None (backend only)

10. **Security/permissions implications?**
    - Secure API key storage in Firebase Functions config, never expose in client code

11. **Dependencies or blocking integrations?**
    - OpenAI account + API key, Pinecone account + index creation

12. **Rollout strategy and metrics?**
    - Deploy to staging first, monitor logs/costs for 24h, then production

13. **What is explicitly out of scope?**
    - User-facing changes, semantic search, RAG pipeline, historical backfill

---

## Authoring Notes

- This is foundation infrastructure - no UI changes
- Embedding generation must not block message delivery (async processing)
- Focus on reliability and error handling (messaging app can't afford failures)
- Cost monitoring critical (OpenAI and Pinecone both usage-based pricing)
- Test with realistic message volume before production deployment
- Reference `Psst/agents/shared-standards.md` for messaging performance requirements
